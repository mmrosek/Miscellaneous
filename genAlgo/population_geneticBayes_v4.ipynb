{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import time\n",
    "import string\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dna_data\n",
    "# from dna_data import DNA # To get this to work, needed to put 'if name == main' at bottom of dna.py \n",
    "# importlib.reload(dna_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to describe a population of virtual organisms\n",
    "# In this case, each organism is just an instance of a DNA object\n",
    "\n",
    "class Population:\n",
    "    def __init__(self, data, preproc_algos, models, ens_methods, tgt, mut_rate, pop_sz, fit_exp, eval_perc, mating_pool_retain_perc, replace=True, midpt=False, verbose=False, debug=False):\n",
    "\n",
    "        self.population = [] \n",
    "        self.mating_pool = [] \n",
    "        self.generations = 0\n",
    "        self.evaluations = 0\n",
    "        self.finished = False \n",
    "        self.mut_rate = mut_rate\n",
    "        self.perfect_score = 0.43\n",
    "        self.best = \"\"\n",
    "        self.fitness_sum = 0\n",
    "        self.fit_exp = fit_exp # Raise fitness to this power to increase (if > 1) prob. of higher fitness members breeding\n",
    "        self.eval_perc = eval_perc/100 # % of members of population to evaluate and replace for 2nd+ generation\n",
    "        self.mating_pool_retain_perc = mating_pool_retain_perc/100 # % of top fitness mems in mating pool to NOT replace\n",
    "        self.replace_bool = replace # True: replace 'num_new_mems' mems of pop w/ lowest fitness w/ new children // False: new children will be appended to existing population\n",
    "        self.verbose = verbose\n",
    "        self.debug = debug\n",
    "        self.eval_idxs = []\n",
    "        \n",
    "        # If not all mems of pop are being evaluated in each generation, can't use fitness for breeding\n",
    "        if self.eval_perc != 1: self.midpt_bool = False # True: Choose point to split mems being bred // False: probabilistically select gene from one of mems being bred based on fitness\n",
    "        else: self.midpt_bool = False\n",
    "            \n",
    "        # NEW - 10/10\n",
    "        if len(preproc_algos) != len(data): self.preproc_algos = []\n",
    "        else: self.preproc_algos = preproc_algos\n",
    "            \n",
    "        # Creating dictionary of dataframes to avoid passing actual df to each DNA instance\n",
    "        self.data_dict = {}\n",
    "        if (type(data) == list) & (len(data) > 0):\n",
    "            count = 0\n",
    "            for df in data:\n",
    "                self.data_dict[count] = df.copy()\n",
    "                count += 1\n",
    "            del data\n",
    "        else: raise Exception(\"No data passed or data passed was not in list form. Need at least dataframe in a list.\")\n",
    "        \n",
    "        # Ensuring models were passed in correct format\n",
    "        if (type(models) == list) & ( len(models) > 0 ): self.models = models\n",
    "        else: raise Exception(\"No model passed or model(s) passed was not in list form. Need at least one model in a list.\")\n",
    "            \n",
    "        self.tgt = tgt # array of ground truths we are trying to predict (i.e. y)\n",
    "                \n",
    "        for i in range(pop_sz):\n",
    "            self.population.append( DNA( list(self.data_dict.keys()), self.preproc_algos, self.models, ens_methods, verbose=self.verbose ) )\n",
    "\n",
    "            \n",
    "    def calc_fitness(self):\n",
    "        '''\n",
    "        Calculates fitness for every member of the population, exponentiates the fitness and calculates population sum\n",
    "        '''\n",
    "        if self.debug: print(\"start of calc_fitness\")\n",
    "        self.fitness_sum = 0\n",
    "    \n",
    "        # For initial population, calculate fitness for every member before generating new members\n",
    "        if self.generations == 0:\n",
    "            \n",
    "            self.eval_idxs = [i for i in range(len(self.population))]\n",
    "            \n",
    "            for i in self.eval_idxs:\n",
    "                self.population[i].calc_fitness(self.data_dict, self.tgt)\n",
    "                self.evaluations += 1\n",
    "                print(f\"Pre exp fitness: {self.population[i].fitness}\")\n",
    "                self.population[i].fitness = self.population[i].fitness**self.fit_exp\n",
    "                print(f\"Post exp fitness: {self.population[i].fitness}\")\n",
    "                self.fitness_sum += self.population[i].fitness\n",
    "                print(f\"fitness sum: {self.fitness_sum}\")\n",
    "                \n",
    "        # For all following generations, evaluate only self.eval_perc % of members\n",
    "        else:\n",
    "            if self.eval_perc == 1: self.eval_idxs = [i for i in range(len(self.population))]\n",
    "                \n",
    "            # If not evaluating all mems of pop, randomly select eval_perc * len(pop) mems to eval\n",
    "            else:\n",
    "                self.eval_idxs = []\n",
    "                while len(self.eval_idxs) < self.eval_perc * len(self.population):\n",
    "                    self.eval_idxs.append(np.random.randint(0, len(self.population)))\n",
    "                    self.eval_idxs = list(set(self.eval_idxs))\n",
    "                    \n",
    "            for i in self.eval_idxs:\n",
    "                self.population[i].calc_fitness(self.data_dict, self.tgt)\n",
    "                self.evaluations += 1\n",
    "                self.population[i].fitness = self.population[i].fitness**self.fit_exp\n",
    "                self.fitness_sum += self.population[i].fitness\n",
    "\n",
    "            \n",
    "    def gen_mating_pool(self):\n",
    "        '''\n",
    "        Generates mating pool as sorted list of tuples (pop_idx, exponentiated_fitness) w/ highest scoring mems first\n",
    "        '''\n",
    "        if self.debug: print(\"start of gen_mating_pool\")\n",
    "        self.mating_pool = []\n",
    "    \n",
    "        for i in range( len(self.eval_idxs) ): \n",
    "    \n",
    "            # Appending (idx, normalized fitness) for each idx in eval_idxs\n",
    "            self.mating_pool.append( (self.eval_idxs[i], self.population[self.eval_idxs[i]].fitness / max(self.fitness_sum, 1e-4) ) )\n",
    "    \n",
    "        # Sorting by fitness score in descending order\n",
    "        self.mating_pool.sort(reverse=True, key = lambda x : x[1])\n",
    "        \n",
    "    \n",
    "    def pick_mem_from_mating_pool(self):\n",
    "        '''\n",
    "        Selects a member (mem) from the mating pool to participate in crossover.\n",
    "\n",
    "        Steps for selection:\n",
    "            1). Draw random number b/w 0-1 (val)\n",
    "            2). Subtract normalized fitness of 1st mem of mating pool (highest fitness) from val\n",
    "            3). If val is now negative, mem of pop corresponding to first mem of mating pool is selected for crossover.\n",
    "            4). If val is still positive, move to 2nd mem of mating pool and repeat until val is negative\n",
    "        '''\n",
    "        if self.debug: print(\"Start of pick_mem_from_mating_pool\")\n",
    "        val = np.random.random()\n",
    "        \n",
    "        if self.verbose: print(f\"val: {val}\")\n",
    "        if self.verbose: print(f\"mating pool: {self.mating_pool}\")\n",
    "#         print(f\"val: {val}\")\n",
    "#         print(f\"mating pool: {self.mating_pool}\")\n",
    "            \n",
    "        for i in range( len(self.mating_pool) ):\n",
    "            val -= self.mating_pool[i][1]\n",
    "            if val < 0:\n",
    "                break\n",
    "        if self.verbose: print(f\"idx of mating pool: {self.mating_pool[i][0]}\")\n",
    "        return self.mating_pool[i][0]\n",
    "    \n",
    "    def pick_mem_from_population(self):\n",
    "        '''\n",
    "        Randomly selects a member (mem) from the population to participate in crossover.\n",
    "        '''            \n",
    "        pop_idx = np.random.randint(0, len(self.population))\n",
    "        return pop_idx\n",
    "    \n",
    "    \n",
    "    def gen_new_pop(self):\n",
    "        '''\n",
    "        Generates new members (mems) of population by probabilistically mating existing mems in the mating pool.\n",
    "        Mems of the mating pool w/ higher fitness are more likely to be selected for mating.\n",
    "        '''\n",
    "        if self.debug: print(\"Start of gen_new_pop\")\n",
    "        children = []\n",
    "        \n",
    "        ### Breeding children ###\n",
    "        # For every member we have evaluated, we need to breed a replacement\n",
    "        num_children = int( len(self.eval_idxs) * (1 - self.mating_pool_retain_perc) )\n",
    "        print(f\"Generating {num_children} children\")\n",
    "        for i in range( num_children ): \n",
    "                \n",
    "            # Selecting members to be bred\n",
    "            idx1 = self.pick_mem_from_mating_pool()\n",
    "            if self.eval_perc == 1: idx2 = self.pick_mem_from_mating_pool()\n",
    "            else: idx2 = self.pick_mem_from_mating_pool()\n",
    "            \n",
    "            #THIS WAS NEW --> would want with low mutation rate\n",
    "            # Sampling a random (top 10 fitness) member if idx1 == idx2, w/ 50% probability\n",
    "            if (idx1 == idx2) & (np.random.random() > 0.5): \n",
    "                rand_hi_idx = np.random.randint( min(10, len(self.mating_pool) ) )\n",
    "                idx2 = self.mating_pool[rand_hi_idx][0]\n",
    "                if self.verbose: print(\"idx1 == idx2 dealt with\")\n",
    "            \n",
    "            partnerA = self.population[idx1]\n",
    "            partnerB = self.population[idx2]\n",
    "            \n",
    "            # Crossover & mutation --> mutants (below) are initialized w/ empty genes for non-[models, data, preproc] genes\n",
    "            child = partnerA.crossover(partnerB, midpt_bool=self.midpt_bool)\n",
    "            child.mutate(self.mut_rate, self.models)\n",
    "            children.append(child)\n",
    "\n",
    "        ### Updating population ###\n",
    "        if self.replace_bool:    \n",
    "            if self.eval_perc == 1:\n",
    "                for i in range(len(children)):\n",
    "                    replace_idx = self.mating_pool[len(self.mating_pool) - i - 1][0]\n",
    "                    self.population[replace_idx] = children[i] # Overwrites self.population[replace_idx].fitness w/ 0\n",
    "            else: # Can use mating pool for replacements if it is not too small\n",
    "                for i in range(len(children)):\n",
    "                    replace_idx = self.mating_pool[len(self.mating_pool) - i - 1][0]\n",
    "                    self.population[replace_idx] = children[i] # Overwrites self.population[replace_idx].fitness w/ 0\n",
    "        else:\n",
    "            self.population.extend(children)\n",
    "\n",
    "        self.generations += 1\n",
    "\n",
    "    def evaluate(self):\n",
    "        '''\n",
    "        Computes the current \"most fit\" member of the population and whether the perfect score has been achieved.\n",
    "        '''\n",
    "        world_record = 0\n",
    "        idx = 0\n",
    "        for i in range(len(self.population)): \n",
    "            if self.population[i].fitness > world_record:\n",
    "                idx = i\n",
    "                world_record = self.population[i].fitness\n",
    "                \n",
    "        print(f\"World Record: {world_record**(1/self.fit_exp)}\\n\")\n",
    "\n",
    "        self.best = self.population[idx]\n",
    "        \n",
    "        for dna in self.population:\n",
    "            print({'Data':dna.genes['data'], 'Preprocessing':dna.genes['preproc'], 'Models':dna.genes['models']})\n",
    "        \n",
    "        if world_record**(1/self.fit_exp) >= self.perfect_score:\n",
    "            self.finished = True\n",
    "            \n",
    "        print(f\"\\nBest: {self.get_best()}\")\n",
    "        print(f\"Average: {self.get_average_fitness()}\")\n",
    "\n",
    "        # If we found the target phrase, stop\n",
    "        if self.is_finished():\n",
    "            print(\"\\nWe did it :)\")\n",
    "            print(f\"Result: {self.get_best(get_all=True)}\")\n",
    "            print(f\"\\nNum gens: {self.get_generations()}\")\n",
    "            print(f\"Num evals: {self.get_evaluations()}\")\n",
    "\n",
    "    def is_finished(self):\n",
    "        return self.finished\n",
    "    \n",
    "    def get_best(self, get_all = False):\n",
    "        return self.best.get_genes(get_all = get_all)\n",
    "    \n",
    "    def get_evaluations(self):\n",
    "        return self.evaluations\n",
    "\n",
    "    def get_generations(self):\n",
    "        return self.generations\n",
    "\n",
    "    def get_average_fitness(self):\n",
    "        total = 0\n",
    "        for i in range( len( self.population ) ):\n",
    "            total += self.population[i].fitness**(1/self.fit_exp)\n",
    "        return total / len(self.population)\n",
    "    \n",
    "    \n",
    "    def evolve(self):\n",
    "    \n",
    "        # Calculate fitness for each mem of pop, take fitness**fit_exp and calc fitness sum\n",
    "        self.calc_fitness()\n",
    "        \n",
    "        # Compute most fit mem of pop and determine if finished\n",
    "        self.evaluate()\n",
    "        \n",
    "        if not self.finished:\n",
    "\n",
    "            # Generate mating pool array by sorting normalized fitness values\n",
    "            self.gen_mating_pool()\n",
    "\n",
    "            # Generate new population mems by crossover b/w existing mems of mating pool\n",
    "            # Either replace existing mems or add new mems to pop\n",
    "            self.gen_new_pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading toy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_X = diabetes.data\n",
    "diabetes_y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cuts = [int(diabetes_X.shape[1] / 4), int(diabetes_X.shape[1] / 2), int(diabetes_X.shape[1] * (3/4))]\n",
    "data = [diabetes_X[:, : feat_cuts[0]], diabetes_X[:, feat_cuts[0] : feat_cuts[1]], \n",
    "        diabetes_X[:, feat_cuts[1] : feat_cuts[2]], diabetes_X[:, feat_cuts[2] : ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = datasets.load_wine()\n",
    "wine_X = wine.data\n",
    "wine_y = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cuts = [int(wine_X.shape[1] / 4), int(wine_X.shape[1] / 2), int(wine_X.shape[1] * (3/4))]\n",
    "data = [wine_X[:, : feat_cuts[0]], wine_X[:, feat_cuts[0] : feat_cuts[1]], \n",
    "        wine[:, feat_cuts[1] : feat_cuts[2]], wine[:, feat_cuts[2] : ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dna class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dna_data_v5\n",
    "from dna_data_v5 import DNA # To get this to work, needed to put 'if name == main' at bottom of dna.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dna_data_v5' from '/home/mjmrose/workspace/sbox-mjmrose/Bids/genAlgo/dna_data_v5.py'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dna_data_v5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.genes['data']: [0, None, None, 3]\n",
      "self.genes['models']: [None, 'lr', 'xgb']\n",
      "self.genes['data']: [None, 1, 2, 3]\n",
      "self.genes['models']: [None, 'lr', None]\n",
      "self.genes['data']: [None, 1, 2, None]\n",
      "self.genes['models']: ['rf', None, None]\n",
      "self.genes['data']: [None, 1, 2, None]\n",
      "self.genes['models']: ['rf', 'lr', 'xgb']\n",
      "self.genes['data']: [0, None, None, 3]\n",
      "self.genes['models']: ['rf', 'lr', 'xgb']\n",
      "self.genes['data']: [None, 1, 2, 3]\n",
      "self.genes['models']: [None, 'lr', 'xgb']\n",
      "self.genes['data']: [0, 1, None, 3]\n",
      "self.genes['models']: ['rf', 'lr', 'xgb']\n",
      "self.genes['data']: [None, 1, 2, 3]\n",
      "self.genes['models']: [None, 'lr', None]\n",
      "self.genes['data']: [None, None, 2, 3]\n",
      "self.genes['models']: ['rf', None, None]\n",
      "self.genes['data']: [0, None, 2, 3]\n",
      "self.genes['models']: ['rf', 'lr', 'xgb']\n"
     ]
    }
   ],
   "source": [
    "preproc_algos = []\n",
    "models = ['rf', 'lr', 'xgb']\n",
    "ens_methods = ['avg', 'lr', 'el_net']\n",
    "tgt = diabetes_y\n",
    "pop_sz = 10\n",
    "eval_perc = 50\n",
    "mating_pool_retain_perc = 10\n",
    "mut_rate = 0.3\n",
    "fit_exp = 2\n",
    "pop = Population(data, preproc_algos, models, ens_methods, tgt, mut_rate, pop_sz, fit_exp, eval_perc, mating_pool_retain_perc, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing speed of convergence w/ gene selection based on fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.29035729488090367\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.356\n",
      "\n",
      "R2 for test_preds: 0.23331235398505412\n",
      "lr_ensembling\n",
      "\n",
      "Score: 0.19639371253636528\n",
      "\n",
      "Pre exp fitness: 0.19639371253636528\n",
      "Post exp fitness: 0.03857049032381648\n",
      "fitness sum: 0.03857049032381648\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.3997840862157056\n",
      "\n",
      "Performing grid search\n",
      "Best score: 0.510\n",
      "\talpha: 0.2\n",
      "\tl1_ratio: 1\n",
      "\n",
      "Score: 0.40594645958535824\n",
      "\n",
      "Pre exp fitness: 0.40594645958535824\n",
      "Post exp fitness: 0.1647925280498869\n",
      "fitness sum: 0.20336301837370338\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.376\n",
      "\n",
      "R2 for test_preds: 0.32228326102754234\n",
      "\n",
      "Performing grid search\n",
      "Best score: 0.510\n",
      "\talpha: 0.2\n",
      "\tl1_ratio: 1\n",
      "\n",
      "Score: 0.3121895095378029\n",
      "\n",
      "Pre exp fitness: 0.3121895095378029\n",
      "Post exp fitness: 0.09746228986545392\n",
      "fitness sum: 0.3008253082391573\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.370\n",
      "\n",
      "R2 for test_preds: 0.3528271456181842\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.4090050787153757\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.387\n",
      "\n",
      "R2 for test_preds: 0.34596193184798507\n",
      "lr_ensembling\n",
      "\n",
      "Score: 0.35746931804786697\n",
      "\n",
      "Pre exp fitness: 0.35746931804786697\n",
      "Post exp fitness: 0.12778431334560708\n",
      "fitness sum: 0.4286096215847644\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.379\n",
      "\n",
      "R2 for test_preds: 0.193961814524651\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.29035729488090367\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.361\n",
      "\n",
      "R2 for test_preds: 0.23462979276239226\n",
      "\n",
      "Performing grid search\n",
      "Best score: 0.436\n",
      "\talpha: 0.2\n",
      "\tl1_ratio: 1\n",
      "\n",
      "Score: 0.23792543305819658\n",
      "\n",
      "Pre exp fitness: 0.23792543305819658\n",
      "Post exp fitness: 0.05660851169593038\n",
      "fitness sum: 0.4852181332806948\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.3997840862157056\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.481\n",
      "\n",
      "R2 for test_preds: 0.38162184231985874\n",
      "lr_ensembling\n",
      "\n",
      "Score: 0.34363182203932063\n",
      "\n",
      "Pre exp fitness: 0.34363182203932063\n",
      "Post exp fitness: 0.11808282911806332\n",
      "fitness sum: 0.6033009623987581\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.436\n",
      "\n",
      "R2 for test_preds: 0.30367990784276544\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.44086804090236564\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.486\n",
      "\n",
      "R2 for test_preds: 0.37973527499627635\n",
      "\n",
      "Performing grid search\n",
      "Best score: 0.735\n",
      "\talpha: 0.2\n",
      "\tl1_ratio: 1\n",
      "\n",
      "Score: 0.3636223789036692\n",
      "\n",
      "Pre exp fitness: 0.3636223789036692\n",
      "Post exp fitness: 0.13222123443956357\n",
      "fitness sum: 0.7355221968383217\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.3997840862157056\n",
      "\n",
      "Score: 0.3997840862157056\n",
      "\n",
      "Pre exp fitness: 0.3997840862157056\n",
      "Post exp fitness: 0.15982731559132674\n",
      "fitness sum: 0.8953495124296484\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.374\n",
      "\n",
      "R2 for test_preds: 0.24089929083453132\n",
      "\n",
      "Score: 0.24089929083453132\n",
      "\n",
      "Pre exp fitness: 0.24089929083453132\n",
      "Post exp fitness: 0.05803246832458011\n",
      "fitness sum: 0.9533819807542285\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.379\n",
      "\n",
      "R2 for test_preds: 0.23933329679019777\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.33737076509207686\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.373\n",
      "\n",
      "R2 for test_preds: 0.2950084907485705\n",
      "\n",
      "Performing grid search\n",
      "Best score: 0.489\n",
      "\talpha: 0.2\n",
      "\tl1_ratio: 1\n",
      "\n",
      "Score: 0.23499187765526686\n",
      "\n",
      "Pre exp fitness: 0.23499187765526686\n",
      "Post exp fitness: 0.05522118256394791\n",
      "fitness sum: 1.0086031633181765\n",
      "World Record: 0.40594645958535824\n",
      "\n",
      "{'Data': [0, None, None, 3], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [None, 1, 2, 3], 'Preprocessing': [], 'Models': [None, 'lr', None]}\n",
      "{'Data': [None, 1, 2, None], 'Preprocessing': [], 'Models': ['rf', None, None]}\n",
      "{'Data': [None, 1, 2, None], 'Preprocessing': [], 'Models': ['rf', 'lr', 'xgb']}\n",
      "{'Data': [0, None, None, 3], 'Preprocessing': [], 'Models': ['rf', 'lr', 'xgb']}\n",
      "{'Data': [None, 1, 2, 3], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [0, 1, None, 3], 'Preprocessing': [], 'Models': ['rf', 'lr', 'xgb']}\n",
      "{'Data': [None, 1, 2, 3], 'Preprocessing': [], 'Models': [None, 'lr', None]}\n",
      "{'Data': [None, None, 2, 3], 'Preprocessing': [], 'Models': ['rf', None, None]}\n",
      "{'Data': [0, None, 2, 3], 'Preprocessing': [], 'Models': ['rf', 'lr', 'xgb']}\n",
      "\n",
      "Best: {'Data': [None, 1, 2, 3], 'Preprocessing': [], 'Models': [None, 'lr', None], 'Ens_Method': 'el_net'}\n",
      "Average: 0.3092853888414083\n",
      "Generating 9 children\n",
      "Gene being mutated: data\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.44086804090236564\n",
      "\n",
      "Performing grid search\n",
      "Best score: 0.518\n",
      "\talpha: 0.2\n",
      "\tl1_ratio: 1\n",
      "\n",
      "Score: 0.4472368125172157\n",
      "\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.455\n",
      "\n",
      "R2 for test_preds: 0.3535042741601464\n",
      "\n",
      "Performing grid search\n",
      "Best score: 0.558\n",
      "\talpha: 0.2\n",
      "\tl1_ratio: 1\n",
      "\n",
      "Score: 0.3427839781732672\n",
      "\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.3997840862157056\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.485\n",
      "\n",
      "R2 for test_preds: 0.3620500448201298\n",
      "\n",
      "Performing grid search\n",
      "Best score: 0.579\n",
      "\talpha: 0.2\n",
      "\tl1_ratio: 1\n",
      "\n",
      "Score: 0.35841736998487117\n",
      "\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.379\n",
      "\n",
      "R2 for test_preds: 0.24132565730626876\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.29035729488090367\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.356\n",
      "\n",
      "R2 for test_preds: 0.2354432933217061\n",
      "\n",
      "Performing grid search\n",
      "Best score: 0.439\n",
      "\talpha: 0.2\n",
      "\tl1_ratio: 1\n",
      "\n",
      "Score: 0.239980069509926\n",
      "\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.468\n",
      "\n",
      "R2 for test_preds: 0.39762055234520854\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.3997840862157056\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.484\n",
      "\n",
      "R2 for test_preds: 0.37757959204662106\n",
      "lr_ensembling\n",
      "\n",
      "Score: 0.34376073772063886\n",
      "\n",
      "World Record: 0.4472368125172157\n",
      "\n",
      "{'Data': [None, 1, 2, None], 'Preprocessing': [], 'Models': ['rf', None, 'xgb']}\n",
      "{'Data': [None, 1, 2, 3], 'Preprocessing': [], 'Models': [None, 'lr', None]}\n",
      "{'Data': [0, 1, None, 3], 'Preprocessing': [], 'Models': [None, 'lr', None]}\n",
      "{'Data': [None, 1, 2, 3], 'Preprocessing': [], 'Models': ['rf', None, None]}\n",
      "{'Data': [None, 1, 2, 3], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [None, 1, 2, 3], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [0, None, None, 3], 'Preprocessing': [], 'Models': ['rf', 'lr', 'xgb']}\n",
      "{'Data': [None, 1, 2, 3], 'Preprocessing': [], 'Models': [None, 'lr', None]}\n",
      "{'Data': [None, 1, 2, 3], 'Preprocessing': [], 'Models': ['rf', 'lr', 'xgb']}\n",
      "{'Data': [None, 1, 2, None], 'Preprocessing': [], 'Models': ['rf', 'lr', None]}\n",
      "\n",
      "Best: {'Data': [0, 1, None, 3], 'Preprocessing': [], 'Models': [None, 'lr', None], 'Ens_Method': 'el_net'}\n",
      "Average: 0.2138125427491277\n",
      "\n",
      "We did it :)\n",
      "Result: {'Data': [0, 1, None, 3], 'Preprocessing': [], 'Models': [None, 'lr', None], 'Ens_Method': 'el_net'}\n",
      "\n",
      "Num gens: 1\n",
      "Num evals: 15\n",
      "\n",
      "Time: 13.827563524246216\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(2000):\n",
    "    pop.evolve()\n",
    "    if pop.is_finished():\n",
    "        break\n",
    "print(f\"\\nTime: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93337761])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop.best.genes['ens_model'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, ps, imps = train_xgb(diabetes_X, diabetes_y, diabetes_X, n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb(X_tr, y_tr, X_te, n_iter = 35, num_folds = 5):\n",
    "    \n",
    "    print(\"XGBoost\")\n",
    "    params = {'max_depth': sp_randint(1,5),\n",
    "              'min_child_weight': sp_randint(1,35),\n",
    "              'learning_rate': uniform(0.06,0.03),\n",
    "              'reg_lambda': uniform(1,1),\n",
    "              'subsample': uniform(0.8,0.2),\n",
    "              'colsample_bytree':uniform(0.8,0.2)}\n",
    "\n",
    "    rs = RandomizedSearchCV(estimator = xgb.XGBRegressor(),\n",
    "        param_distributions=params, cv = num_folds, n_jobs = 24, n_iter = n_iter)\n",
    "\n",
    "    print(\"\\nPerforming randomized search\")\n",
    "    try: rs.fit(X_tr, y_tr)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pdb.set_trace()\n",
    "    print(\"Best score: %0.3f\" % rs.best_score_)\n",
    "    best_parameters = rs.best_estimator_.get_params()\n",
    "    preds = rs.predict(X_te)\n",
    "    feat_imps = \n",
    "    \n",
    "#     # Add features whose importance are 0\n",
    "#     for i in range(x_train.shape[1]):\n",
    "#         key = 'f' + str(i)\n",
    "#         if key not in importances_raw.keys(): ## The importance of this feature is 0\n",
    "#             importances_raw[key] = 0\n",
    "    \n",
    "    return preds, best_parameters, feat_imps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
