{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import time\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dna_data\n",
    "# from dna_data import DNA # To get this to work, needed to put 'if name == main' at bottom of dna.py \n",
    "# importlib.reload(dna_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to describe a population of virtual organisms\n",
    "# In this case, each organism is just an instance of a DNA object\n",
    "\n",
    "class Population:\n",
    "    def __init__(self, data, preproc_algos, models, tgt, mut_rate, pop_sz, fit_exp, eval_perc, mating_pool_retain_perc, replace=True, midpt=False, verbose=False, debug=False):\n",
    "\n",
    "        self.population = [] \n",
    "        self.mating_pool = [] \n",
    "        self.generations = 0\n",
    "        self.evaluations = 0\n",
    "        self.finished = False \n",
    "        self.mut_rate = mut_rate\n",
    "        self.perfect_score = 0.43\n",
    "        self.best = \"\"\n",
    "        self.fitness_sum = 0\n",
    "        self.fit_exp = fit_exp # Raise fitness to this power to increase (if > 1) prob. of higher fitness members breeding\n",
    "        self.eval_perc = eval_perc/100 # % of members of population to evaluate and replace for 2nd+ generation\n",
    "        self.mating_pool_retain_perc = mating_pool_retain_perc/100 # % of top fitness mems in mating pool to NOT replace\n",
    "        self.replace_bool = replace # True: replace 'num_new_mems' mems of pop w/ lowest fitness w/ new children // False: new children will be appended to existing population\n",
    "        self.verbose = verbose\n",
    "        self.debug = debug\n",
    "        self.eval_idxs = []\n",
    "        \n",
    "        # If not all mems of pop are being evaluated in each generation, can't use fitness for breeding\n",
    "        if self.eval_perc != 1: self.midpt_bool = False # True: Choose point to split mems being bred // False: probabilistically select gene from one of mems being bred based on fitness\n",
    "        else: self.midpt_bool = False\n",
    "            \n",
    "        # NEW - 10/10\n",
    "        if len(preproc_algos) != len(data): self.preproc_algos = []\n",
    "        else: self.preproc_algos = preproc_algos\n",
    "            \n",
    "        # Creating dictionary of dataframes to avoid passing actual df to each DNA instance\n",
    "        self.data_dict = {}\n",
    "        if (type(data) == list) & (len(data) > 0):\n",
    "            count = 0\n",
    "            for df in data:\n",
    "                self.data_dict[count] = df.copy()\n",
    "                count += 1\n",
    "            del data\n",
    "        else: raise Exception(\"No data passed or data passed was not in list form. Need at least dataframe in a list.\")\n",
    "        \n",
    "        # Ensuring models were passed in correct format\n",
    "        if (type(models) == list) & ( len(models) > 0 ): self.models = models\n",
    "        else: raise Exception(\"No model passed or model(s) passed was not in list form. Need at least one model in a list.\")\n",
    "            \n",
    "        self.tgt = tgt # array of ground truths we are trying to predict (i.e. y)\n",
    "                \n",
    "        for i in range(pop_sz):\n",
    "            self.population.append( DNA( list(self.data_dict.keys()), self.preproc_algos, self.models, verbose=self.verbose ) )\n",
    "\n",
    "            \n",
    "    def calc_fitness(self):\n",
    "        '''\n",
    "        Calculates fitness for every member of the population, exponentiates the fitness and calculates population sum\n",
    "        '''\n",
    "        if self.debug: print(\"start of calc_fitness\")\n",
    "        self.fitness_sum = 0\n",
    "    \n",
    "        # For initial population, calculate fitness for every member before generating new members\n",
    "        if self.generations == 0:\n",
    "            \n",
    "            self.eval_idxs = [i for i in range(len(self.population))]\n",
    "            \n",
    "            for i in self.eval_idxs:\n",
    "                self.population[i].calc_fitness(self.data_dict, self.tgt)\n",
    "                self.evaluations += 1\n",
    "                print(f\"Pre exp fitness: {self.population[i].fitness}\")\n",
    "                self.population[i].fitness = self.population[i].fitness**self.fit_exp\n",
    "                print(f\"Post exp fitness: {self.population[i].fitness}\")\n",
    "                self.fitness_sum += self.population[i].fitness\n",
    "                print(f\"fitness sum: {self.fitness_sum}\")\n",
    "                \n",
    "        # For all following generations, evaluate only self.eval_perc % of members\n",
    "        else:\n",
    "            if self.eval_perc == 1: self.eval_idxs = [i for i in range(len(self.population))]\n",
    "                \n",
    "            # If not evaluating all mems of pop, randomly select eval_perc * len(pop) mems to eval\n",
    "            else:\n",
    "                self.eval_idxs = []\n",
    "                while len(self.eval_idxs) < self.eval_perc * len(self.population):\n",
    "                    self.eval_idxs.append(np.random.randint(0, len(self.population)))\n",
    "                    self.eval_idxs = list(set(self.eval_idxs))\n",
    "                    \n",
    "            for i in self.eval_idxs:\n",
    "                self.population[i].calc_fitness(self.data_dict, self.tgt)\n",
    "                self.evaluations += 1\n",
    "                self.population[i].fitness = self.population[i].fitness**self.fit_exp\n",
    "                self.fitness_sum += self.population[i].fitness\n",
    "\n",
    "            \n",
    "    def gen_mating_pool(self):\n",
    "        '''\n",
    "        Generates mating pool as sorted list of tuples (pop_idx, exponentiated_fitness) w/ highest scoring mems first\n",
    "        '''\n",
    "        if self.debug: print(\"start of gen_mating_pool\")\n",
    "        self.mating_pool = []\n",
    "    \n",
    "        for i in range( len(self.eval_idxs) ): \n",
    "    \n",
    "            # Appending (idx, normalized fitness) for each idx in eval_idxs\n",
    "            self.mating_pool.append( (self.eval_idxs[i], self.population[self.eval_idxs[i]].fitness / max(self.fitness_sum, 1e-4) ) )\n",
    "    \n",
    "        # Sorting by fitness score in descending order\n",
    "        self.mating_pool.sort(reverse=True, key = lambda x : x[1])\n",
    "        \n",
    "    \n",
    "    def pick_mem_from_mating_pool(self):\n",
    "        '''\n",
    "        Selects a member (mem) from the mating pool to participate in crossover.\n",
    "\n",
    "        Steps for selection:\n",
    "            1). Draw random number b/w 0-1 (val)\n",
    "            2). Subtract normalized fitness of 1st mem of mating pool (highest fitness) from val\n",
    "            3). If val is now negative, mem of pop corresponding to first mem of mating pool is selected for crossover.\n",
    "            4). If val is still positive, move to 2nd mem of mating pool and repeat until val is negative\n",
    "        '''\n",
    "        if self.debug: print(\"Start of pick_mem_from_mating_pool\")\n",
    "        val = np.random.random()\n",
    "        \n",
    "        if self.verbose: print(f\"val: {val}\")\n",
    "        if self.verbose: print(f\"mating pool: {self.mating_pool}\")\n",
    "#         print(f\"val: {val}\")\n",
    "#         print(f\"mating pool: {self.mating_pool}\")\n",
    "            \n",
    "        for i in range( len(self.mating_pool) ):\n",
    "            val -= self.mating_pool[i][1]\n",
    "            if val < 0:\n",
    "                break\n",
    "        if self.verbose: print(f\"idx of mating pool: {self.mating_pool[i][0]}\")\n",
    "        return self.mating_pool[i][0]\n",
    "    \n",
    "    def pick_mem_from_population(self):\n",
    "        '''\n",
    "        Randomly selects a member (mem) from the population to participate in crossover.\n",
    "        '''            \n",
    "        pop_idx = np.random.randint(0, len(self.population))\n",
    "        return pop_idx\n",
    "    \n",
    "    \n",
    "    def gen_new_pop(self):\n",
    "        '''\n",
    "        Generates new members (mems) of population by probabilistically mating existing mems in the mating pool.\n",
    "        Mems of the mating pool w/ higher fitness are more likely to be selected for mating.\n",
    "        '''\n",
    "        if self.debug: print(\"Start of gen_new_pop\")\n",
    "        children = []\n",
    "        \n",
    "        ### Breeding children ###\n",
    "        # For every member we have evaluated, we need to breed a replacement\n",
    "        num_children = int( len(self.eval_idxs) * (1 - self.mating_pool_retain_perc) )\n",
    "        print(f\"Generating {num_children} children\")\n",
    "        for i in range( num_children ): \n",
    "                \n",
    "            # Selecting members to be bred\n",
    "            idx1 = self.pick_mem_from_mating_pool()\n",
    "            if self.eval_perc == 1: idx2 = self.pick_mem_from_mating_pool()\n",
    "            else: idx2 = self.pick_mem_from_mating_pool()\n",
    "            \n",
    "            #THIS WAS NEW --> would want with low mutation rate\n",
    "            # Sampling a random (top 10 fitness) member if idx1 == idx2, w/ 50% probability\n",
    "            if (idx1 == idx2) & (np.random.random() > 0.5): \n",
    "                rand_hi_idx = np.random.randint( min(10, len(self.mating_pool) ) )\n",
    "                idx2 = self.mating_pool[rand_hi_idx][0]\n",
    "                if self.verbose: print(\"idx1 == idx2 dealt with\")\n",
    "            \n",
    "            partnerA = self.population[idx1]\n",
    "            partnerB = self.population[idx2]\n",
    "            \n",
    "            # Crossover and mutation\n",
    "            child = partnerA.crossover(partnerB, midpt_bool=self.midpt_bool)\n",
    "            child.mutate(self.mut_rate, self.models)\n",
    "            children.append(child)\n",
    "\n",
    "        ### Updating population ###\n",
    "        if self.replace_bool:    \n",
    "            if self.eval_perc == 1:\n",
    "                for i in range(len(children)):\n",
    "                    replace_idx = self.mating_pool[len(self.mating_pool) - i - 1][0]\n",
    "                    self.population[replace_idx] = children[i] # Overwrites self.population[replace_idx].fitness w/ 0\n",
    "            else: # Can use mating pool for replacements if it is not too small\n",
    "                for i in range(len(children)):\n",
    "                    replace_idx = self.mating_pool[len(self.mating_pool) - i - 1][0]\n",
    "                    self.population[replace_idx] = children[i] # Overwrites self.population[replace_idx].fitness w/ 0\n",
    "        else:\n",
    "            self.population.extend(children)\n",
    "\n",
    "        self.generations += 1\n",
    "\n",
    "    def evaluate(self):\n",
    "        '''\n",
    "        Computes the current \"most fit\" member of the population and whether the perfect score has been achieved.\n",
    "        '''\n",
    "        world_record = 0\n",
    "        idx = 0\n",
    "        for i in range(len(self.population)): \n",
    "            if self.population[i].fitness > world_record:\n",
    "                idx = i\n",
    "                world_record = self.population[i].fitness\n",
    "                \n",
    "        print(f\"World Record: {world_record**(1/self.fit_exp)}\\n\")\n",
    "\n",
    "        self.best = self.population[idx]\n",
    "        \n",
    "        for dna in self.population:\n",
    "            print({'Data':dna.genes['data'], 'Preprocessing':dna.genes['preproc'], 'Models':dna.genes['models']})\n",
    "        \n",
    "        if world_record**(1/self.fit_exp) >= self.perfect_score:\n",
    "            self.finished = True\n",
    "            \n",
    "        print(f\"\\nBest: {self.get_best()}\")\n",
    "        print(f\"Average: {self.get_average_fitness()}\")\n",
    "\n",
    "        # If we found the target phrase, stop\n",
    "        if self.is_finished():\n",
    "            print(\"\\nWe did it :)\")\n",
    "            print(f\"Result: {self.get_best(get_all=True)}\")\n",
    "            print(f\"\\nNum gens: {self.get_generations()}\")\n",
    "            print(f\"Num evals: {self.get_evaluations()}\")\n",
    "\n",
    "    def is_finished(self):\n",
    "        return self.finished\n",
    "    \n",
    "    def get_best(self, get_all = False):\n",
    "        return self.best.get_genes(get_all = get_all)\n",
    "    \n",
    "    def get_evaluations(self):\n",
    "        return self.evaluations\n",
    "\n",
    "    def get_generations(self):\n",
    "        return self.generations\n",
    "\n",
    "    def get_average_fitness(self):\n",
    "        total = 0\n",
    "        for i in range( len( self.population ) ):\n",
    "            total += self.population[i].fitness**(1/self.fit_exp)\n",
    "        return total / len(self.population)\n",
    "    \n",
    "    \n",
    "    def evolve(self):\n",
    "    \n",
    "        # Calculate fitness for each mem of pop, take fitness**fit_exp and calc fitness sum\n",
    "        self.calc_fitness()\n",
    "        \n",
    "        # Compute most fit mem of pop and determine if finished\n",
    "        self.evaluate()\n",
    "        \n",
    "        if not self.finished:\n",
    "\n",
    "            # Generate mating pool array by sorting normalized fitness values\n",
    "            self.gen_mating_pool()\n",
    "\n",
    "            # Generate new population mems by crossover b/w existing mems of mating pool\n",
    "            # Either replace existing mems or add new mems to pop\n",
    "            self.gen_new_pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_X = diabetes.data\n",
    "diabetes_y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dna_data_v4\n",
    "from dna_data_v4 import DNA # To get this to work, needed to put 'if name == main' at bottom of dna.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dna_data_v4' from '/home/mjmrose/workspace/sbox-mjmrose/Bids/genAlgo/dna_data_v4.py'>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dna_data_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.genes['data']: [None, 1, None, None]\n",
      "self.genes['models']: ['lr', None, None]\n",
      "self.genes['data']: [None, None, 2, 3]\n",
      "self.genes['models']: [None, 'lr', 'xgb']\n",
      "self.genes['data']: [None, None, 2, None]\n",
      "self.genes['models']: [None, 'lr', 'xgb']\n",
      "self.genes['data']: [None, None, None, 3]\n",
      "self.genes['models']: ['rf', 'lr', 'xgb']\n",
      "self.genes['data']: [0, 1, None, None]\n",
      "self.genes['models']: [None, 'lr', 'xgb']\n",
      "self.genes['data']: [0, None, 2, 3]\n",
      "self.genes['models']: ['lr', None, None]\n",
      "self.genes['data']: [0, None, None, None]\n",
      "self.genes['models']: ['rf', None, None]\n",
      "self.genes['data']: [None, None, 2, 3]\n",
      "self.genes['models']: ['rf', None, 'xgb']\n",
      "self.genes['data']: [0, 1, 2, None]\n",
      "self.genes['models']: [None, 'lr', 'xgb']\n",
      "self.genes['data']: [0, 1, None, None]\n",
      "self.genes['models']: [None, 'lr', 'xgb']\n"
     ]
    }
   ],
   "source": [
    "feat_cuts = [int(diabetes_X.shape[1] / 4), int(diabetes_X.shape[1] / 2), int(diabetes_X.shape[1] * (3/4))]\n",
    "data = [diabetes_X[:, : feat_cuts[0]], diabetes_X[:, feat_cuts[0] : feat_cuts[1]], \n",
    "        diabetes_X[:, feat_cuts[1] : feat_cuts[2]], diabetes_X[:, feat_cuts[2] : ]]\n",
    "preproc_algos = []\n",
    "models = ['rf', 'lr', 'xgb']\n",
    "tgt = diabetes_y\n",
    "pop_sz = 10\n",
    "eval_perc = 50\n",
    "mating_pool_retain_perc = 10\n",
    "mut_rate = 0.3\n",
    "fit_exp = 2\n",
    "pop = Population(data, preproc_algos, models, tgt, mut_rate, pop_sz, fit_exp, eval_perc, mating_pool_retain_perc, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing speed of convergence w/ gene selection based on fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.3433889528671302\n",
      "\n",
      "Score: 0.3433889528671302\n",
      "\n",
      "Pre exp fitness: 0.3433889528671302\n",
      "Post exp fitness: 0.11791597295118418\n",
      "fitness sum: 0.11791597295118418\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.2958413175999277\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.385\n",
      "\n",
      "R2 for test_preds: 0.2651925265241757\n",
      "\n",
      "Score: 0.29103555380783874\n",
      "\n",
      "Pre exp fitness: 0.29103555380783874\n",
      "Post exp fitness: 0.0847016935802354\n",
      "fitness sum: 0.20261766653141958\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.1922986663450169\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.142\n",
      "\n",
      "R2 for test_preds: 0.1951196168178394\n",
      "\n",
      "Score: 0.20117744271833338\n",
      "\n",
      "Pre exp fitness: 0.20117744271833338\n",
      "Post exp fitness: 0.04047236345868831\n",
      "fitness sum: 0.24309002999010787\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.377\n",
      "\n",
      "R2 for test_preds: 0.24454874932898563\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.2681956825269114\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.369\n",
      "\n",
      "R2 for test_preds: 0.2248149568662341\n",
      "\n",
      "Score: 0.25657049621581396\n",
      "\n",
      "Pre exp fitness: 0.25657049621581396\n",
      "Post exp fitness: 0.06582841952842901\n",
      "fitness sum: 0.30891844951853686\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.35084408475214546\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.341\n",
      "\n",
      "R2 for test_preds: 0.2991105718316883\n",
      "\n",
      "Score: 0.33279744403076095\n",
      "\n",
      "Pre exp fitness: 0.33279744403076095\n",
      "Post exp fitness: 0.11075413875340746\n",
      "fitness sum: 0.4196725882719443\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.33737076509207686\n",
      "\n",
      "Score: 0.33737076509207686\n",
      "\n",
      "Pre exp fitness: 0.33737076509207686\n",
      "Post exp fitness: 0.11381903313881331\n",
      "fitness sum: 0.5334916214107577\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.015\n",
      "\n",
      "R2 for test_preds: -0.06247269748169382\n",
      "\n",
      "Score: -0.06247269748169382\n",
      "\n",
      "Pre exp fitness: -0.06247269748169382\n",
      "Post exp fitness: 0.003902837930639233\n",
      "fitness sum: 0.537394459341397\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.389\n",
      "\n",
      "R2 for test_preds: 0.23561412208204247\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.385\n",
      "\n",
      "R2 for test_preds: 0.2675306825110836\n",
      "\n",
      "Score: 0.25882929536158783\n",
      "\n",
      "Pre exp fitness: 0.25882929536158783\n",
      "Post exp fitness: 0.06699260413737607\n",
      "fitness sum: 0.604387063478773\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.44401824877932017\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.395\n",
      "\n",
      "R2 for test_preds: 0.31524095407181507\n",
      "\n",
      "Score: 0.397251574848847\n",
      "\n",
      "Pre exp fitness: 0.397251574848847\n",
      "Post exp fitness: 0.15780881371988909\n",
      "fitness sum: 0.7621958771986621\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.35084408475214546\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.343\n",
      "\n",
      "R2 for test_preds: 0.3109262063443303\n",
      "\n",
      "Score: 0.33797764703111255\n",
      "\n",
      "Pre exp fitness: 0.33797764703111255\n",
      "Post exp fitness: 0.1142288898926873\n",
      "fitness sum: 0.8764247670913494\n",
      "World Record: 0.397251574848847\n",
      "\n",
      "{'Data': [None, 1, None, None], 'Preprocessing': [], 'Models': ['lr', None, None]}\n",
      "{'Data': [None, None, 2, 3], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [None, None, 2, None], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [None, None, None, 3], 'Preprocessing': [], 'Models': ['rf', 'lr', 'xgb']}\n",
      "{'Data': [0, 1, None, None], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [0, None, 2, 3], 'Preprocessing': [], 'Models': ['lr', None, None]}\n",
      "{'Data': [0, None, None, None], 'Preprocessing': [], 'Models': ['rf', None, None]}\n",
      "{'Data': [None, None, 2, 3], 'Preprocessing': [], 'Models': ['rf', None, 'xgb']}\n",
      "{'Data': [0, 1, 2, None], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [0, 1, None, None], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "\n",
      "Best: {'Data': [0, 1, 2, None], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "Average: 0.2818871869455195\n",
      "Generating 9 children\n",
      "Gene being mutated: data\n",
      "Gene being mutated: models\n",
      "Gene being mutated: models\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.35084408475214546\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.343\n",
      "\n",
      "R2 for test_preds: 0.3137577868559176\n",
      "\n",
      "Score: 0.3390553383182131\n",
      "\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.3433889528671302\n",
      "\n",
      "Score: 0.3433889528671302\n",
      "\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.350\n",
      "\n",
      "R2 for test_preds: 0.26612490421015744\n",
      "\n",
      "Score: 0.26612490421015744\n",
      "\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.44401824877932017\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.403\n",
      "\n",
      "R2 for test_preds: 0.3353611215830402\n",
      "\n",
      "Score: 0.4070416124468704\n",
      "\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.374\n",
      "\n",
      "R2 for test_preds: 0.3001769775849472\n",
      "\n",
      "Score: 0.3001769775849472\n",
      "\n",
      "World Record: 0.4070416124468704\n",
      "\n",
      "{'Data': [0, 1, None, None], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [None, None, 2, None], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [None, 1, None, None], 'Preprocessing': [], 'Models': [None, 'lr', None]}\n",
      "{'Data': [0, 1, 2, 3], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [None, None, None, 3], 'Preprocessing': [], 'Models': ['rf', None, 'xgb']}\n",
      "{'Data': [None, 1, 2, None], 'Preprocessing': [], 'Models': [None, None, 'xgb']}\n",
      "{'Data': [None, 1, None, None], 'Preprocessing': [], 'Models': ['rf', None, None]}\n",
      "{'Data': [None, None, None, 3], 'Preprocessing': [], 'Models': ['rf', 'lr', 'xgb']}\n",
      "{'Data': [0, 1, 2, None], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [0, None, 2, 3], 'Preprocessing': [], 'Models': ['xgb', None, None]}\n",
      "\n",
      "Best: {'Data': [0, 1, 2, None], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "Average: 0.16557877854273184\n",
      "Generating 4 children\n",
      "Gene being mutated: preproc\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.327\n",
      "\n",
      "R2 for test_preds: 0.2892897327934193\n",
      "\n",
      "Score: 0.2892897327934193\n",
      "\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.385\n",
      "\n",
      "R2 for test_preds: 0.22060813935266232\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.371\n",
      "\n",
      "R2 for test_preds: 0.2549599377914321\n",
      "\n",
      "Score: 0.24427418863422357\n",
      "\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.388\n",
      "\n",
      "R2 for test_preds: 0.3663080160469049\n",
      "\n",
      "Score: 0.3663080160469049\n",
      "\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.3433889528671302\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.350\n",
      "\n",
      "R2 for test_preds: 0.28844038760669344\n",
      "\n",
      "Score: 0.3231695274316311\n",
      "\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.44401824877932017\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.399\n",
      "\n",
      "R2 for test_preds: 0.2988321920325785\n",
      "\n",
      "Score: 0.39028198665572333\n",
      "\n",
      "World Record: 0.39028198665572333\n",
      "\n",
      "{'Data': [None, 1, 2, None], 'Preprocessing': [], 'Models': [None, 'lr', None]}\n",
      "{'Data': [None, None, 2, None], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [0, 1, None, None], 'Preprocessing': [], 'Models': ['rf', None, None]}\n",
      "{'Data': [0, 1, 2, 3], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [None, None, None, 3], 'Preprocessing': [], 'Models': ['rf', None, 'xgb']}\n",
      "{'Data': [None, 1, 2, None], 'Preprocessing': [], 'Models': [None, None, 'xgb']}\n",
      "{'Data': [None, 1, None, None], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [None, None, None, 3], 'Preprocessing': [], 'Models': ['rf', 'lr', 'xgb']}\n",
      "{'Data': [0, 1, 2, None], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [0, 1, None, None], 'Preprocessing': [], 'Models': ['rf', 'lr', None]}\n",
      "\n",
      "Best: {'Data': [0, 1, 2, None], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "Average: 0.1613323451561902\n",
      "Generating 4 children\n",
      "Gene being mutated: models\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.4090050787153757\n",
      "\n",
      "Score: 0.4090050787153757\n",
      "\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.389\n",
      "\n",
      "R2 for test_preds: 0.30368222988767735\n",
      "\n",
      "Score: 0.30368222988767735\n",
      "\n",
      "Linear regression\n",
      "\n",
      "R2 for test_preds: 0.4399387660024645\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.489\n",
      "\n",
      "R2 for test_preds: 0.39238949643877374\n",
      "\n",
      "Score: 0.43134413404123184\n",
      "\n",
      "Random Forest\n",
      "Performing randomized search\n",
      "Best score: 0.337\n",
      "\n",
      "R2 for test_preds: 0.3003523290248412\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.348\n",
      "\n",
      "R2 for test_preds: 0.29235780705083736\n",
      "\n",
      "Score: 0.3062744381701956\n",
      "\n",
      "XGBoost\n",
      "Performing randomized search\n",
      "Best score: 0.352\n",
      "\n",
      "R2 for test_preds: 0.2987194014367123\n",
      "\n",
      "Score: 0.2987194014367123\n",
      "\n",
      "World Record: 0.43134413404123184\n",
      "\n",
      "{'Data': [None, 1, 2, None], 'Preprocessing': [], 'Models': [None, 'lr', None]}\n",
      "{'Data': [None, None, 2, None], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [None, 1, 2, None], 'Preprocessing': [], 'Models': [None, None, 'xgb']}\n",
      "{'Data': [0, 1, 2, 3], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [None, 1, None, None], 'Preprocessing': [], 'Models': [None, 'rf', 'xgb']}\n",
      "{'Data': [None, 1, None, None], 'Preprocessing': [], 'Models': [None, None, 'xgb']}\n",
      "{'Data': [None, None, 2, 3], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [None, None, None, 3], 'Preprocessing': [], 'Models': ['rf', 'lr', 'xgb']}\n",
      "{'Data': [0, 1, 2, None], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "{'Data': [0, 1, None, None], 'Preprocessing': [], 'Models': ['rf', 'lr', None]}\n",
      "\n",
      "Best: {'Data': [0, 1, 2, 3], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb']}\n",
      "Average: 0.2139307268906916\n",
      "\n",
      "We did it :)\n",
      "Result: {'Data': [0, 1, 2, 3], 'Preprocessing': [], 'Models': [None, 'lr', 'xgb'], 'Params': [None, array([  -9.16088483, -205.46225988,  516.68462383,  340.62734108,\n",
      "       -895.54360867,  561.21453306,  153.88478595,  126.73431596,\n",
      "        861.12139955,   52.41982836]), {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.811966494318024, 'gamma': 0, 'learning_rate': 0.08635720973599559, 'max_delta_step': 0, 'max_depth': 2, 'min_child_weight': 17, 'missing': None, 'n_estimators': 100, 'n_jobs': 1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 2.8285304389325674, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 0.8392823520053112}], 'Feature Importances': [None, array([  -9.16088483, -205.46225988,  516.68462383,  340.62734108,\n",
      "       -895.54360867,  561.21453306,  153.88478595,  126.73431596,\n",
      "        861.12139955,   52.41982836]), array([0.07526882, 0.05017921, 0.16845877, 0.13978495, 0.07885305,\n",
      "       0.10035843, 0.07168459, 0.01792115, 0.18637992, 0.11111111],\n",
      "      dtype=float32)]}\n",
      "\n",
      "Num gens: 3\n",
      "Num evals: 25\n",
      "\n",
      "Time: 14.778857707977295\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(2000):\n",
    "    pop.evolve()\n",
    "    if pop.is_finished():\n",
    "        break\n",
    "print(f\"\\nTime: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Data': [0, 1, 2, 3],\n",
       " 'Models': ['rf', 'lr', 'xgb'],\n",
       " 'Params': [None,\n",
       "  None,\n",
       "  None,\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'mse',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 6,\n",
       "   'max_leaf_nodes': None,\n",
       "   'min_impurity_decrease': 0.0,\n",
       "   'min_impurity_split': None,\n",
       "   'min_samples_leaf': 17,\n",
       "   'min_samples_split': 2,\n",
       "   'min_weight_fraction_leaf': 0.0,\n",
       "   'n_estimators': 27,\n",
       "   'n_jobs': 1,\n",
       "   'oob_score': False,\n",
       "   'random_state': None,\n",
       "   'verbose': 0,\n",
       "   'warm_start': False},\n",
       "  array([  -9.16088483, -205.46225988,  516.68462383,  340.62734108,\n",
       "         -895.54360867,  561.21453306,  153.88478595,  126.73431596,\n",
       "          861.12139955,   52.41982836]),\n",
       "  {'base_score': 0.5,\n",
       "   'booster': 'gbtree',\n",
       "   'colsample_bylevel': 1,\n",
       "   'colsample_bytree': 0.9416813031933384,\n",
       "   'gamma': 0,\n",
       "   'learning_rate': 0.07031603310519619,\n",
       "   'max_delta_step': 0,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 23,\n",
       "   'missing': None,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': 1,\n",
       "   'nthread': None,\n",
       "   'objective': 'reg:linear',\n",
       "   'random_state': 0,\n",
       "   'reg_alpha': 0,\n",
       "   'reg_lambda': 1.3838440709165318,\n",
       "   'scale_pos_weight': 1,\n",
       "   'seed': None,\n",
       "   'silent': True,\n",
       "   'subsample': 0.8754820592443371}],\n",
       " 'Preprocessing': []}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop.best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "\n",
      "Performing randomized search\n",
      "Best score: 0.445\n"
     ]
    }
   ],
   "source": [
    "preds, ps, imps = train_xgb(diabetes_X, diabetes_y, diabetes_X, n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb(X_tr, y_tr, X_te, n_iter = 35, num_folds = 5):\n",
    "    \n",
    "    print(\"XGBoost\")\n",
    "    params = {'max_depth': sp_randint(1,5),\n",
    "              'min_child_weight': sp_randint(1,35),\n",
    "              'learning_rate': uniform(0.06,0.03),\n",
    "              'reg_lambda': uniform(1,1),\n",
    "              'subsample': uniform(0.8,0.2),\n",
    "              'colsample_bytree':uniform(0.8,0.2)}\n",
    "\n",
    "    rs = RandomizedSearchCV(estimator = xgb.XGBRegressor(),\n",
    "        param_distributions=params, cv = num_folds, n_jobs = 24, n_iter = n_iter)\n",
    "\n",
    "    print(\"\\nPerforming randomized search\")\n",
    "    try: rs.fit(X_tr, y_tr)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pdb.set_trace()\n",
    "    print(\"Best score: %0.3f\" % rs.best_score_)\n",
    "    best_parameters = rs.best_estimator_.get_params()\n",
    "    preds = rs.predict(X_te)\n",
    "    feat_imps = \n",
    "    \n",
    "#     # Add features whose importance are 0\n",
    "#     for i in range(x_train.shape[1]):\n",
    "#         key = 'f' + str(i)\n",
    "#         if key not in importances_raw.keys(): ## The importance of this feature is 0\n",
    "#             importances_raw[key] = 0\n",
    "    \n",
    "    return preds, best_parameters, feat_imps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
